# Database
DATABASE_URL=sqlite+aiosqlite:///./m_suite.db

# JWT
SECRET_KEY=your-secret-key-change-in-production

# =============================================================================
# LLM Settings
# =============================================================================

# LLM Provider: "cloud", "local", "mock", "auto"
# - cloud: Alibaba Cloud Qwen API
# - local: Ollama (ローカルLLM)
# - mock: モックレスポンス（テスト用）
# - auto: 自動選択（APIキーあり→cloud、Ollama起動中→local、それ以外→mock）
LLM_PROVIDER=auto

# フォールバック機能: true/false
# 有効にすると、プライマリプロバイダーが失敗した場合に次のプロバイダーを試行
LLM_FALLBACK_ENABLED=true

# =============================================================================
# Qwen Cloud API (Alibaba Cloud)
# =============================================================================
QWEN_API_KEY=sk-xxxxxxxxxxxxxxxx
# QWEN_API_BASE=https://dashscope.aliyuncs.com/compatible-mode/v1
# QWEN_MODEL=qwen-plus
# QWEN_MODEL_FAST=qwen-turbo
# QWEN_MAX_TOKENS=2048
# QWEN_TEMPERATURE=0.7
# QWEN_TIMEOUT=30

# =============================================================================
# Ollama (Local LLM) - Metal/GPU対応
# =============================================================================
# Ollama server URL
# - ローカル開発: http://localhost:11434
# - Docker環境: http://host.docker.internal:11434 (ホストのOllamaに接続)
#
# Mac環境でGPU(Metal)を活用するには、Ollamaをネイティブインストール:
#   brew install ollama && ollama serve && ollama pull qwen3:1.7b
OLLAMA_BASE_URL=http://localhost:11434

# 使用するモデル
# 推奨モデル:
# - qwen3:0.6b  (~500MB, 1GB RAM) - 超軽量
# - qwen3:1.7b  (~1.5GB, 3GB RAM) - バランス良好（推奨）
# - qwen3:4b    (~3GB, 6GB RAM)   - 高品質
# - qwen3:8b    (~6GB, 10GB RAM)  - 最高品質（GPU推奨）
OLLAMA_MODEL=qwen3:1.7b
OLLAMA_MODEL_FAST=qwen3:1.7b

# Ollamaオプション
# OLLAMA_TIMEOUT=60
# OLLAMA_NUM_CTX=4096
# OLLAMA_TEMPERATURE=0.7
